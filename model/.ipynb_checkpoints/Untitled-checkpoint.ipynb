{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"0\"\n",
    "\n",
    "class MyIOError(Exception):\n",
    "    def __init__(self, filename):\n",
    "        message = \"IOError: Unable to locate file{}\".format(filename)\n",
    "        super(MyIOError, self).__init__(message)\n",
    "        \n",
    "class PreprocessData(object):\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None, m_iteration=None):\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.m_iteration = m_iteration\n",
    "        self.length = None\n",
    "        \n",
    "    \n",
    "    def _pad_sequences(sequences, pad_tok, max_length):\n",
    "        sequence_padded = []\n",
    "        sequence_length = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            seq = list(seq)\n",
    "            seq_ = seq[:max_length] + pad_tok*max(max_length, len(seq),0)\n",
    "            sequence_padded += [seq_]\n",
    "            sequence_length += [min(len(seq), max_length)]\n",
    "            return sequence_padded, sequence_length\n",
    "    def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "        if nlevels == 1 :\n",
    "            max_length = max(map(lambda x: len(x), sequences))\n",
    "            sequence_padded, sequence_length = _pad_sequences(sequences, pad_tok, max_length)\n",
    "        elif nlevels == 2 :\n",
    "            max_length_word = max([max(map(lambda x: len(x), seq)) \n",
    "                              for seq in sequences])\n",
    "            sequence_padded = []\n",
    "            sequence_length = []\n",
    "            for seq in sequences:\n",
    "                sp, s1 = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "                sequence_padded += [sp]\n",
    "                sequence_length += [s1]\n",
    "            \n",
    "            max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "            \n",
    "            sequence_padded, _ = _pad_sequences(sequence_padded, [pad_tok]*max_length_word, max_length_sentence)\n",
    "            sequence_length, _ = _pad_sequences(sequence_length, 0 , max_length_sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Progbar(object):\n",
    "    def __init__(self, target, width=30, verbose=1):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.verbose = verbose\n",
    "        self.sum_values = {}\n",
    "        self.unique_value = {}\n",
    "        self.start = time.time()\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.log = config.log\n",
    "        self.session = None\n",
    "        self.save = None\n",
    "    \n",
    "    def initialize_weights(self, scope):\n",
    "        variables = tf.contrib.framework.get_variables(scope)\n",
    "        init = tf.variables_initializer(variables)\n",
    "        self.session.run(init)\n",
    "    \n",
    "    def add_train_op(self, method, lr_rate, loss, clip=-1):\n",
    "        _m = method.lower()\n",
    "        \n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _m == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(lr_rate)\n",
    "            elif _m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr_rate)\n",
    "            elif _m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr_rate)\n",
    "            elif _m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr_rate)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_m))\n",
    "            \n",
    "            if clip > 0 :\n",
    "                gd, vs = zip(*optimizer.compute_gradients(loss))\n",
    "                gd, gnorm = tf.clip_by_norm(gd, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(gd, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss)\n",
    "        \n",
    "    def initialize_session(self):\n",
    "        self.log.info(\"Initialize tf session\")\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.save = tf.train.Saver()\n",
    "        \n",
    "    def restore_session(self, dir_model):\n",
    "        self.log.info(\"Reload the latest trained model...\")\n",
    "        self.save.restore(self.session, dir_model)\n",
    "    \n",
    "    def save_session(self):\n",
    "        if not os.path.exists(self.config.dir_model):\n",
    "            os.makedirs(self.config.dir_model)\n",
    "        self.save.save(self.session, self.config.dir_model)\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.session.close()\n",
    "\n",
    "    \n",
    "    def add_summary(self):\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config,dir_output, self.session.graph)\n",
    "        \n",
    "    def train(self,train, dev):\n",
    "        record = 0\n",
    "        num_epoch_no_imprv = 0\n",
    "        self.add_summary()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.log.info(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
    "                        self.config.num_epochs))\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            self.config.lr_rate *= self.config.lr_decay\n",
    "            \n",
    "            if score >= record:\n",
    "                num_epoch_no_imprv = 0\n",
    "                self.save_session()\n",
    "                record = score\n",
    "                self.log.info(\"New best score recorded...\")\n",
    "            else:\n",
    "                num_epoch_no_imprv += 1\n",
    "                if num_epoch_no_imprv >= self.config.num_epoch_no_imprv:\n",
    "                    self.log.info(\"Early stopping {} epochs without \"\\\n",
    "                            \"improvement\".format(num_epoch_no_imprv))\n",
    "                    break            \n",
    "                    \n",
    "    def evaluate(self, test):\n",
    "        self.log.info(\"Evaluating on test set\")\n",
    "        metrics = self.run_evaluate(test)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.log.info(msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseModel import Model\n",
    "class NERModel(Model):\n",
    "    def __init__(self, config):\n",
    "        super(NERModel, self).__init__(config)\n",
    "        self.index_to_tag = {idx: tag for tag, idx in self.config.vocab_tag.items()}\n",
    "        \n",
    "    def initialize_placeholder_tensor(self):\n",
    "        self.word_id = tf.placeholder(tf.int32, shape=[None, None], name=\"word_id\")\n",
    "        self.sequence_length = tf.placeholder(tf.int32, shape=[None], name=\"sequence_length\")\n",
    "        self.char_id = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_id\")\n",
    "        self.word_length = tf.placeholder(tf.int32, shapnge=[None, None], name=\"word_length\")\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, None], name=\"label\")\n",
    "        self.drop_out = tf.placeholder(tf.float32, shape=[], name=\"drop_out\")\n",
    "        self.lr_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        \n",
    "    def feed_dict(self, word, label=None, lr_rate=None, drop_out=None):\n",
    "        if self.config.use_chars:\n",
    "            char_id, word_id = zip(*word)\n",
    "            word_id, sequence_length = pad_sequences(word_id, 0)\n",
    "            char_id, word_length = pad_sequences(char_id, pad_tok=0, nlevel=2)\n",
    "        else: \n",
    "            word_id , sequence_length = pad_sequences(word, 0)\n",
    "        \n",
    "        feed = {self.word_id: word_id, self.sequence_length: sequence_length}\n",
    "        \n",
    "        if self.config.use_chars:\n",
    "            feed[self.char_id] = char_id\n",
    "            feed[self.word_length] = word_length\n",
    "            \n",
    "        if label is not None:\n",
    "            label, _ = pad_sequences(label, 0)\n",
    "            feed[self.label] = label\n",
    "        \n",
    "        if lr_rate is not None:\n",
    "            feed[self.lr_rate] = lr_rate\n",
    "        \n",
    "        if drop_out is not None:\n",
    "            feed[self.drop_out] = drop_out\n",
    "        \n",
    "        return feed, sequence_length\n",
    "\n",
    "    def word_embbeding_option(self):\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            if self.config.embbedings is None:\n",
    "                self.log.info(\"WARNING: randomly initializing word vectors\")\n",
    "                _word_embbedings = tf.get_variable(name=\"_word_embbedings\",dtype=tf.float32,shape=[self.config.nwords, self.config.dim_word])\n",
    "                \n",
    "            else:\n",
    "                _word_embbedings = tf.Variable(self.config.embbedings, name=\"_word_embbedings\", dtype=tf.float32, trainable=self.config.embbedings)\n",
    "                \n",
    "            word_embbedings = tf.nn.embedding_lookup(_word_embbedings, self.word_id, name=\"word_embbeding\")\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                _char_embbedings = tf.get_variable(name=\"_char_embbeding\", dtype=tf.float32,shape=[self.config.nchars, self.config.dim_char])\n",
    "                char_embbedings = tf.nn.embedding_lookup(_char_embbedings, self.char_id, name=\"char_embbedings\")\n",
    "                s = tf.shape(char_embbedings)\n",
    "                char_embbedings = tf.reshape(char_embbedings, shape=[s[0]*s[1],s[-2], self.config.dim_char])\n",
    "                word_lengths = tf.reshape(self.word_length, shape=[s[0]]*s[1])\n",
    "                #define bi-LSTM neural network\n",
    "                \n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char, state_is_tuple=True)\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char, state_is_tuple=True)\n",
    "                _output = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=char_embbedings,sequence_length=word_lengths, dtype=tf.float32)\n",
    "                \n",
    "                _, ((_, output_fw),(_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis=1)\n",
    "                \n",
    "                output = tf.reshape(output, shape=[s[0]*s[1],2*self.config.hidden_size_char])\n",
    "                word_embbedings = tf.concat([word_embbedings,output], axis=1)\n",
    "                \n",
    "        self.word_embbedings = tf.nn.dropout(word_embbedings, self.drop_out)\n",
    "    \n",
    "    def logits_option(self):\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            \n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw, cell_bw=cell_bw, inputs=self.word_embbedings, sequence_length=self.sequence_length, dtype=tf.float32)\n",
    "            \n",
    "            output = tf.concat([output_fw, output_bw], axis=1)\n",
    "            output = tf.nn.dropout(output, self.drop_out)\n",
    "            \n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32, shape=[2*self.confog.hidden_size_lstm, self.config.num_tags])\n",
    "            b= tf.get_variable(\"b\", dtype=tf.float32, shape=[self.config.n_tags],initializer=tf.zeros_initializer())\n",
    "            num_steps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, shape=[-1,2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, num_steps, self.config.num_tags])\n",
    "            \n",
    "    def prediction_option(self):\n",
    "        if not self.config.use_crf:\n",
    "            self.label_pred = tf.cast(tf.argmax(self.logits,axis=-1), tf.int32)\n",
    "            \n",
    "    def loss_option(self):        \n",
    "        if self.config.use_crf:\n",
    "            log_similar, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.label, self.sequence_length)\n",
    "            self.trans_params = trans_params\n",
    "            self.loss = tf.reduce_mean(-log_similar)\n",
    "            \n",
    "        else:\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.label)\n",
    "            mask = tf.sequence_mask(self.sequence_length)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        \n",
    "    def build(self):\n",
    "        self.initialize_placeholder_tensor()\n",
    "        self.word_embbeding_option()\n",
    "        self.logits_option()\n",
    "        self.prediction_option()\n",
    "        self.loss_option()\n",
    "        \n",
    "        self.add_train_op(self.config.method, self.lr_rate, self.loss, self.config.clip)\n",
    "        self.initialize_session()\n",
    "        \n",
    "        \n",
    "    def predict_batch(self,words):\n",
    "        fd, sequence_length = self.feed_dict(words, drop_out=1.0)\n",
    "        if self.config.use_crf:\n",
    "            bi_sequences = []\n",
    "            loggits , trans_params = self.session.run([self.logits, self.trans_params], feed_dict=fd)\n",
    "            \n",
    "            for lg, seq_len in zip(logits, sequence_length):\n",
    "                logit = logit[:seq_len]\n",
    "                bi_seq , bi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n",
    "                bi_seqs += [bi_seq]\n",
    "            \n",
    "            return bi_seqs, sequence_length\n",
    "        \n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        batch_size = self.config.batch_size\n",
    "        num_batch = (len(train) + batch_size -1) // batch_size\n",
    "        prog = Progbar(target=num_batch)\n",
    "        \n",
    "        for i, (word, label) in enumerate(minibatches(train, batch_size)):\n",
    "            fd , _ = self.feed_dict(word, label, self.config.lr_rate, self.config.drop_out)\n",
    "            _, train_loss, summary = self.session.run([self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "            \n",
    "            prog.update(i+1, [(\"train loss\", train_loss)])\n",
    "            if (i%10 == 0):\n",
    "                self.file_writer.add_summary(summary, epoch*num_batch+i)\n",
    "                \n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def evaluate(self, test):\n",
    "        accs = []\n",
    "        correct_pred. total_correct, total_pred = 0.,0.,0.\n",
    "        for word, label in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(word)\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
