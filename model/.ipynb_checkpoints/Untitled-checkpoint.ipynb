{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"0\"\n",
    "\n",
    "class MIOError(Exception):\n",
    "    def __init__(self, filename):\n",
    "        message = \"IOError: Unable to locate file{}\".format(filename)\n",
    "        super(MIOError, self).__init__(message)\n",
    "        \n",
    "class PreProcessData(object):\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None, m_iteration=None):\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.m_iteration = m_iteration\n",
    "        self.length = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        num_iter = 0\n",
    "        \n",
    "        with open(self.filename) as f:\n",
    "            word = []\n",
    "            tag = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(word) != 0:\n",
    "                        num_iter +=1\n",
    "                        if self.m_iteration is not None and num_iter > self.m_iteration:\n",
    "                            break\n",
    "                        yield word, tag \n",
    "                        word, tag = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    w, t = ls[0], ls[-1]\n",
    "                    if self.processing_word is not None:\n",
    "                        w = self.processing_word(w)\n",
    "                    if self.processing_tag is not None:\n",
    "                        t = self.processing_tag(t)\n",
    "                    word += [w]\n",
    "                    tag += [t]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for  _ in self:\n",
    "                self.length += 1\n",
    "        \n",
    "        return self.length\n",
    "    \n",
    "    def processing_vocab(data):\n",
    "        print(\"Building Vocabulary...\")\n",
    "        w_vocab = set()\n",
    "        t_vocab = set()\n",
    "        for d in data:\n",
    "            for word, tag in data:\n",
    "                w_vocab.update(word)\n",
    "                t_vocab.update(tag)\n",
    "        print(\"- done. {} tokens\".format(len(w_vocab)))\n",
    "        return w_vocab, t_vocab\n",
    "    \n",
    "    def processing_char_vocab(data):\n",
    "        c_vocab = set()\n",
    "        for word, _ in data:\n",
    "            c_vocab.update(word)\n",
    "        return c_vocab\n",
    "    \n",
    "    def glove_vocab(filename):\n",
    "        print(\"Buidling Glove Vocabulary ...\")\n",
    "        glove_vocab = set()\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                word = line.strip().split(' ')[0]\n",
    "                glove_vocab.add(word)\n",
    "        print(\" -done. {} tokens\".format(len(glove_vocab)))\n",
    "        \n",
    "    def writing_vocab(vocab, filename):\n",
    "        print(\"Writing output file...\")\n",
    "        with open(filename,\"w\") as f:\n",
    "            for i, word in enumerate(vocab):\n",
    "                if i != len(vocab) -1:\n",
    "                    f.write(\"{}\\n\".format(word))\n",
    "                else:\n",
    "                    f.write(word)\n",
    "        print(\" - done. {} tokens\". format(len(vocab)))\n",
    "    \n",
    "    def load_dict(filename):\n",
    "        try:\n",
    "            d = dict()\n",
    "            with open(filename) as f:\n",
    "                for idx, word in enumerate(f):\n",
    "                    word = word.strip()\n",
    "                    d[word] = idx\n",
    "        except IOError:\n",
    "            raise MIOError(filename)\n",
    "        return d\n",
    "    \n",
    "    def exp_trimmed_glove_vector(vocab, glove_filename, trimmed_filename, dim):\n",
    "        embeddings = np.zeros([len(vocab), dim])\n",
    "        with open(glove_filename) as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(' ')\n",
    "                word = line[0]\n",
    "                embeddings = [float(x) for x in line[1:]]\n",
    "                if word in vocab:\n",
    "                    w_idx = vocab[word]\n",
    "                    embeddings[w_idx] = np.asarray(embeddings)\n",
    "        np.savez_compressed(trimmed_filename, embeddings=embeddings)\n",
    "    \n",
    "    def processing_trimmed_glove_vector(filename):\n",
    "        try:\n",
    "            with np.load(filename) as data:\n",
    "                return data[\"embeddings\"]\n",
    "        except IOError:\n",
    "            raise MIOError(filename)\n",
    "            \n",
    "    def get_processing_word(vocab_words=None, vocab_chars=None, \n",
    "                            lowercase=False, chars=False, allow_unk=True):\n",
    "        def f(word):\n",
    "            if vocab_words is not None and chars == True:\n",
    "                c_id =[]\n",
    "                for char in word:\n",
    "                    if char in vocab_chars:\n",
    "                        c_id += [vocab_chars[char]]\n",
    "            if lowercase:\n",
    "                word.lower()\n",
    "            if word.digit():\n",
    "                word = NUM\n",
    "            \n",
    "            if vocab_words is not None:\n",
    "                if word in vocab_words:\n",
    "                    word = vocab_words[word]\n",
    "                else:\n",
    "                    if allow_unk:\n",
    "                        word = vocab_words[UNK]\n",
    "                    else:\n",
    "                        raise Exception(\"UnKnown Key. Please re-check the tag\")\n",
    "            \n",
    "            if vocab_chars is not None and chars == True:\n",
    "                return c_id, word\n",
    "            else:\n",
    "                return word\n",
    "        return f\n",
    "    \n",
    "    def _pad_sequences(sequences, pad_tok, max_length):\n",
    "        sequence_padded = []\n",
    "        sequence_length = []\n",
    "        for seq in sequences:\n",
    "            seq = list(seq)\n",
    "            seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq),0)\n",
    "            sequence_padded += [seq_]\n",
    "            sequence_length += [min(len(seq), max_length)]\n",
    "            return sequence_padded, sequence_length\n",
    "    def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "        if nlevels == 1 :\n",
    "            max_length = max(map(lambda x: len(x), sequences))\n",
    "            sequence_padded, sequence_length = _pad_sequences(sequences, pad_tok, max_length)\n",
    "        elif nlevels == 2 :\n",
    "            max_length_word = max([max(map(lambda x: len(x), seq)) for seq in sequences])\n",
    "            sequence_padded = []\n",
    "            sequence_length = []\n",
    "            for seq in sequences:\n",
    "                sp, s1 = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "                sequence_padded += [sp]\n",
    "                sequence_length += [s1]\n",
    "            \n",
    "            max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "            \n",
    "            sequence_padded, _ = _pad_sequences(sequence_padded, [pad_tok]*max_length_word, max_length_sentence)\n",
    "            sequence_length, _ = _pad_sequences(sequence_length, 0 , max_length_sentence)\n",
    "        \n",
    "        return sequence_padded, sequence_length\n",
    "    def minibatches(data, size):\n",
    "        b_x, b_y = [],[]\n",
    "        for (x,y) in data:\n",
    "            if len(b_x) == size:\n",
    "                yield b_x, b_y\n",
    "                b_x, b_y = [],[]\n",
    "            \n",
    "            if type(x[0]) == tuple:\n",
    "                x = zip(*x)\n",
    "            b_x += [x]\n",
    "            b_y += [y]\n",
    "            \n",
    "        if len(b_x) != 0:\n",
    "            yield b_x, b_y\n",
    "    \n",
    "    def get_chunk_type(token,tag):\n",
    "        tag_name = tag[token]\n",
    "        tag_class = tag_name.split('-')[0]\n",
    "        tag_type = tag_name.split('-')[1]\n",
    "        return tag_class, tag_type\n",
    "    \n",
    "    def get_chunks(seq, tags):\n",
    "        default = tags[NONE]\n",
    "        tag_id = {idx: tag for tag, idx in tags.items()}\n",
    "        chunk_list = []\n",
    "        chunk_type, chunk_start = None,None\n",
    "        for i, token in enumerate(seq):\n",
    "            if token == default and chunk_type is not None:\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunk_list.append(chunk)\n",
    "                chunk_type, chunk_start = None, None\n",
    "            elif token != default:\n",
    "                token_chunk_class, token_chunk_type = get_chunk_type(token, tag_id)\n",
    "                if chunk_type is None:\n",
    "                    chunk_type, chunk_start = token_chunk_type, i\n",
    "                elif token_chunk_type != chunk_type or token_chunk_class == \"B\"\n",
    "                    chunk = (chunk_type, chunk_start, i)\n",
    "                    chunk_list.append(chunk)\n",
    "                    chunk_type, chunk_start = token_chunk_type, i\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if chunk_type is not None:\n",
    "            chunk = (chunk_type, chunk_start, len(seq))\n",
    "            chunk_list.append(chunk)\n",
    "        return chunk_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def logging_file(filename):\n",
    "    logger = logging.getLogger('logger')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "    handler = logging.FileHandler(filename)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
    "    logging.getLogger().addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "class Progress(object):\n",
    "    def __init__(self, target, width=30, verbose=1):\n",
    "        self.width = width\n",
    "        self.target = target\n",
    "        self.verbose = verbose\n",
    "        self.sum_value = {}\n",
    "        self.unique_value = {}\n",
    "        self.start = time.time()\n",
    "        self.total_width = 0\n",
    "        self.seen_so_far = 0\n",
    "        \n",
    "    def update(self, current, value=[], exact=[], strict=[]):\n",
    "        for k,v in value:\n",
    "            if k not in self.sum_value:\n",
    "                self.sum_value[k] =[v*(current - self.seen_so_far), current - self.seen_sofar]\n",
    "                self.unique_value.append(k)\n",
    "            else:\n",
    "                self.sum_value[k][0] += v*(current - self.seen_so_far)\n",
    "                self.sum_value[k][1] += (current - self.seen_so_far)\n",
    "        \n",
    "        for k, v in exact:\n",
    "            if k not in self.sum_value:\n",
    "                self.unique_value.append(k)\n",
    "            self.sum_value[k] = v\n",
    "        \n",
    "        self.seen_so_far = current\n",
    "        \n",
    "        now = time.time()\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            prev_total_width = self.total_width\n",
    "            sys.stdout.write(\"\\b\" * prev_total_width)\n",
    "            sys.stdout.write(\"\\r\")\n",
    "            \n",
    "            num_digit = int(np.floor(np.log10(self.target))) +1 \n",
    "            string_bar = '%%%dd/%%%dd [' % (num_digit, num_digit)\n",
    "            bar = string_bar % (current, self.target)\n",
    "            prog = float(current)/ self.target\n",
    "            prog_width = int(self.width*prog)\n",
    "            if prog_width > 0:\n",
    "                bar += ('='*(prog_width-1))\n",
    "                if current < self.target:\n",
    "                    bar += '>'\n",
    "                else:\n",
    "                    bar += '='\n",
    "            bar += ('.'*(self.width-prog_width))\n",
    "            bar += ']'\n",
    "            sys.stdout.write(bar)\n",
    "            self.total_width = len(bar)\n",
    "            \n",
    "            if current:\n",
    "                time_per_unit = (now - self.start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            eta = time_per_unit*(self.target - current)\n",
    "            info = ''\n",
    "            if current < self.target:\n",
    "                info += ' -ETA: %ds '%eta\n",
    "            else:\n",
    "                info += ' -%ds' %(now - self.start)\n",
    "\n",
    "            for k in self.unique_value:\n",
    "                if type(self.sum_value[k][0]) is list:\n",
    "                    info += ' -%s: %.4f ' %(k, self.sum_value[k][0] / max(1, self.sum_value[k][1]))\n",
    "                else:\n",
    "                    info += ' -%s: %s ' %(k, self.sum_value[k])\n",
    "\n",
    "            self.total_width += len(info)\n",
    "\n",
    "            if prev_total_width > self.total_width:\n",
    "                info += ((prev_total_width-self.total_width)*\" \")\n",
    "            sys.stdout.write(info)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if current > self.target: \n",
    "                sys.stdout.write(\"\\n\")\n",
    "        \n",
    "        if self.verbose == 2:\n",
    "            if current >= self.target:\n",
    "                info = '%ds' %(now-self.start)\n",
    "                \n",
    "                for k in self.unique_value:\n",
    "                    info += '- %s: %.4f' %(k, self.sum_value[k][0] /  max(1, self.sum_value[k][1]))\n",
    "                    \n",
    "                sys.stdout.write(info + \"\\n\")\n",
    "        \n",
    "    def add(self, n, value=[]):\n",
    "        self.update(self.seen_so_far+n, value)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.log = config.log\n",
    "        self.session = None\n",
    "        self.save = None\n",
    "    \n",
    "    def initialize_weights(self, scope):\n",
    "        variables = tf.contrib.framework.get_variables(scope)\n",
    "        init = tf.variables_initializer(variables)\n",
    "        self.session.run(init)\n",
    "    \n",
    "    def add_train_op(self, method, lr_rate, loss, clip=-1):\n",
    "        _m = method.lower()\n",
    "        \n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            if _m == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(lr_rate)\n",
    "            elif _m == 'adagrad':\n",
    "                optimizer = tf.train.AdagradOptimizer(lr_rate)\n",
    "            elif _m == 'sgd':\n",
    "                optimizer = tf.train.GradientDescentOptimizer(lr_rate)\n",
    "            elif _m == 'rmsprop':\n",
    "                optimizer = tf.train.RMSPropOptimizer(lr_rate)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown method {}\".format(_m))\n",
    "            \n",
    "            if clip > 0 :\n",
    "                gd, vs = zip(*optimizer.compute_gradients(loss))\n",
    "                gd, gnorm = tf.clip_by_norm(gd, clip)\n",
    "                self.train_op = optimizer.apply_gradients(zip(gd, vs))\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(loss)\n",
    "        \n",
    "    def initialize_session(self):\n",
    "        self.log.info(\"Initialize tf session\")\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.save = tf.train.Saver()\n",
    "        \n",
    "    def restore_session(self, dir_model):\n",
    "        self.log.info(\"Reload the latest trained model...\")\n",
    "        self.save.restore(self.session, dir_model)\n",
    "    \n",
    "    def save_session(self):\n",
    "        if not os.path.exists(self.config.dir_model):\n",
    "            os.makedirs(self.config.dir_model)\n",
    "        self.save.save(self.session, self.config.dir_model)\n",
    "    \n",
    "    def close_session(self):\n",
    "        self.session.close()\n",
    "\n",
    "    \n",
    "    def add_summary(self):\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config,dir_output, self.session.graph)\n",
    "        \n",
    "    def train(self,train, dev):\n",
    "        record = 0\n",
    "        num_epoch_no_imprv = 0\n",
    "        self.add_summary()\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            self.log.info(\"Epoch {:} out of {:}\".format(epoch + 1,\n",
    "                        self.config.num_epochs))\n",
    "            score = self.run_epoch(train, dev, epoch)\n",
    "            self.config.lr_rate *= self.config.lr_decay\n",
    "            \n",
    "            if score >= record:\n",
    "                num_epoch_no_imprv = 0\n",
    "                self.save_session()\n",
    "                record = score\n",
    "                self.log.info(\"New best score recorded...\")\n",
    "            else:\n",
    "                num_epoch_no_imprv += 1\n",
    "                if num_epoch_no_imprv >= self.config.num_epoch_no_imprv:\n",
    "                    self.log.info(\"Early stopping {} epochs without \"\\\n",
    "                            \"improvement\".format(num_epoch_no_imprv))\n",
    "                    break            \n",
    "                    \n",
    "    def evaluate(self, test):\n",
    "        self.log.info(\"Evaluating on test set\")\n",
    "        metrics = self.run_evaluate(test)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.log.info(msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from model import Model\n",
    "from utils import Progress\n",
    "from data_utils import minibatches, pad_sequences, get_chunks\n",
    "\n",
    "class NERModel(Model):\n",
    "    def __init__(self, config):\n",
    "        super(NERModel, self).__init__(config)\n",
    "        self.tag_idx = {idx: tag for tag, idx in self.config.vocab_list.items()}\n",
    "        \n",
    "    def initialize_placeholder_tensor(self):\n",
    "        self.c_id = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_id\") # [batch_size, max_length_sentence, max_length_word]\t\t\t\t\n",
    "        self.w_id = tf.placeholder(tf.int32, shape=[None, None], name=\"word_id\") #[batch_size, max_length_of_sentence_in_batch]\n",
    "        self.w_len = tf.placeholder(tf.int32, shapnge=[None, None], name=\"word_len\")  # [batch_size, max_length_sentence]\n",
    "        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=\"sequence_length\") #[batch_size]\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, None], name=\"label\") # [batch size, max_length_of_sentence_in_batch]\n",
    "        self.drop_out = tf.placeholder(tf.float32, shape=[], name=\"drop_out\")\n",
    "        self.lr_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        \n",
    "    def feed_dict(self, word, label=None, lr_rate=None, drop_out=None):\n",
    "        if self.config.use_chars:\n",
    "            c_id, w_id = zip(*word)\n",
    "            w_id, seq_len = pad_sequences(w_id, 0)\n",
    "            c_id, w_len = pad_sequences(c_id, pad_tok=0, nlevel=2)\n",
    "        else: \n",
    "            w_id , seq_len = pad_sequences(word, 0)\n",
    "        \n",
    "        feed = {self.w_id: w_id, self.seq_len: seq_len}\n",
    "        \n",
    "        if self.config.use_chars:\n",
    "            feed[self.c_id] = c_id\n",
    "            feed[self.w_len] = w_len\n",
    "            \n",
    "        if label is not None:\n",
    "            label, _ = pad_sequences(label, 0)\n",
    "            feed[self.label] = label\n",
    "        \n",
    "        if lr_rate is not None:\n",
    "            feed[self.lr_rate] = lr_rate\n",
    "        \n",
    "        if drop_out is not None:\n",
    "            feed[self.drop_out] = drop_out\n",
    "        \n",
    "        return feed, seq_len\n",
    "\n",
    "    def word_embbeding_option(self):\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            if self.config.embbedings is None:\n",
    "                self.log.info(\"WARNING: randomly initializing word vectors\")\n",
    "                _word_embbeding = tf.get_variable(name=\"_word_embbeding\",dtype=tf.float32,shape=[self.config.num_word, self.config.dim_word])\n",
    "            else:\n",
    "                _word_embbeding = tf.Variable(self.config.embbedings, name=\"_word_embbeding\", dtype=tf.float32, trainable=self.config.train_embbedings)\n",
    "                \n",
    "            word_embbedings = tf.nn.embedding_lookup(_word_embbeding, self.w_id, name=\"word_embbeding\")\n",
    "        \n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                _char_embbeding = tf.get_variable(name=\"_char_embbeding\", dtype=tf.float32, shape=[self.config.num_char, self.config.dim_char])\n",
    "                char_embbedings = tf.nn.embedding_lookup(_char_embbedings, self.c_id, name=\"char_embbeding\")\n",
    "                s = tf.shape(char_embbedings)\n",
    "\n",
    "                char_embbedings = tf.reshape(char_embbedings, shape=[s[0]*s[1],s[-2], self.config.dim_char])\n",
    "\n",
    "                w_len = tf.reshape(self.w_len, shape=[s[0]*s[1]])\n",
    "\n",
    "                #define bi-LSTM\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char, state_is_tuple=True)\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char, state_is_tuple=True)\n",
    "                _output = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw,char_embbedings, sequence_length=w_len, dtype=tf.float32)\n",
    "                \n",
    "                _, ((_, output_fw),(_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "                \n",
    "                output = tf.reshape(output, shape=[s[0],s[1],2*self.config.hidden_size_char])\n",
    "                word_embbeding = tf.concat([word_embbeding,output], axis=-1)\n",
    "                \n",
    "        self.word_embbeding = tf.nn.dropout(word_embbeding, self.drop_out)\n",
    "    \n",
    "    def logits_option(self):\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            \n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, self.word_embbeding, sequence_length=self.seq_len, dtype=tf.float32)\n",
    "            \n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.drop_out)\n",
    "            \n",
    "        with tf.variable_scope(\"projection\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32, shape=[2*self.config.hidden_size_lstm, self.config.n_tag])\n",
    "            b= tf.get_variable(\"b\", dtype=tf.float32,  shape=[self.config.n_tag], initializer=tf.zeros_initializer())\n",
    "            num_step = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, shape=[-1,2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logit = tf.reshape(pred, [-1, num_step, self.config.n_tag])\n",
    "            \n",
    "    def prediction_option(self):\n",
    "        if not self.config.use_crf:\n",
    "            self.label_pred = tf.cast(tf.argmax(self.logit,axis=-1), tf.int32)\n",
    "            \n",
    "    def loss_option(self):        \n",
    "        if self.config.use_crf:\n",
    "            log_similar, trans_params = tf.contrib.crf.crf_log_likelihood(self.logit, self.label, self.seq_len)\n",
    "            self.trans_params = trans_params\n",
    "            self.loss = tf.reduce_mean(-log_similar)       \n",
    "        else:\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logit, labels=self.label)\n",
    "            mask = tf.sequence_mask(self.seq_len)\n",
    "            loss = tf.boolean_mask(loss, mask)\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        \n",
    "    def build(self):\n",
    "        self.initialize_placeholder_tensor()\n",
    "        self.word_embbeding_option()\n",
    "        self.logits_option()\n",
    "        self.prediction_option()\n",
    "        self.loss_option()\n",
    "        \n",
    "        self.add_train_op(self.config.method, self.lr_rate, self.loss, self.config.clip)\n",
    "        self.initialize_session()\n",
    "        \n",
    "        \n",
    "    def predict_batch(self,word):\n",
    "        fd, seq_len = self.feed_dict(word, drop_out=1.0)\n",
    "        if self.config.use_crf:\n",
    "            viterbi_seq = []\n",
    "            logit , trans_params = self.session.run([self.logit, self.trans_params], feed_dict=fd)            \n",
    "            for lg, sl in zip(logit, seq_len):\n",
    "                lg = lg[:sl]\n",
    "               \tvi_seq , vi_score = tf.contrib.crf.viterbi_decode(lg, trans_params)\n",
    "                viterbi_seq += [vi_seq]\n",
    "            \n",
    "            return viterbi_seq, seq_len\n",
    "        \n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        batch_size = self.config.batch_size\n",
    "        num_batch = (len(train) + batch_size -1) // batch_size\n",
    "        prog = Progress(target=num_batch)\n",
    "        \n",
    "        for i, (word, label) in enumerate(minibatches(train, batch_size)):\n",
    "            fd , _ = self.feed_dict(word, label, self.config.lr_rate, self.config.drop_out)\n",
    "            _, train_loss, summary = self.session.run([self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "            \n",
    "            prog.update(i+1, [(\"train loss\", train_loss)])\n",
    "            if (i%10 == 0):\n",
    "                self.file_writer.add_summary(summary, epoch*num_batch+i)\n",
    "                \n",
    "        metric = self.evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v) for k, v in metrics.items()])\n",
    "        self.log.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "    \n",
    "    def evaluate(self, test):\n",
    "        accuracy = []\n",
    "        correct_prediction = 0.\n",
    "        total_correct = 0.\n",
    "        total_prediction = 0.\n",
    "        for word, label in minibatches(test, self.config.batch_size):\n",
    "            label_predict, seq_len = self.predict_batch(word)\n",
    "\n",
    "        for lb, lb_pred, length in zip(label, label_predict, seq_len):\n",
    "            lb = lb[:length]\n",
    "            lb_pred = lb_pred[:length]\n",
    "            accuracy += [a==b for (a,b) in zip(lb, lb_pred)]\n",
    "            lb_chunks = set(get_chunks(lb, self.config.vocab_list))\n",
    "            lb_pred_chunks = set(get_chunks(lb_pred, self.config.vocab_list))\n",
    "            correct_prediction += len(lb_chunks & lb_pred_chunks)\n",
    "            total_prediction += len(lb_pred_chunks)\n",
    "            total_correct += len(lb_chunks)\n",
    "            \n",
    "        \n",
    "        precision = correct_prediction / total_prediction if correct_prediction >0 else 0\n",
    "        recall = correct_prediction / total_correct if correct_prediction >0 else 0\n",
    "        f1 = 2*precision*recall / (precision+recall) if correct_prediction >0 else 0\n",
    "        acc = np.mean(accuracy)\n",
    "        \n",
    "        return {\"accuracy\": 100*acc, \"f1-score\": 100*f1}\n",
    "    \n",
    "    def predict(self, raw_word):\n",
    "        \n",
    "        word = [self.config.processing_word(w) for w in raw_word]\n",
    "        if type(word[0]) == tuple:\n",
    "            word = zip(*word)\n",
    "        p_id, _ = self.predict_batch([word])\n",
    "        prediction = [self.tag_idx[idx] for idx in list(p_id[0])]\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.bidirectional_dynamic_rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
